{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Book Genre Classification with Naive Bayes and BERT \n",
    "author: Julia Joy & Oscar Fleet\n",
    "date: '2024-04-18'\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this install once, for experimenting with hugging face's transformers for BERT\n",
    "# CPU usage, just to ensure that model architecture will be working.\n",
    "# Will later likely need to use GPUs using a virtual environment,\n",
    "# IF: our dataset is too big, possibly not the case.\n",
    "#pip install transformers[torch]\n",
    "\n",
    "# Same here, run once\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports need necessary installs found above\n",
    "from naiveModel import NBLangIDModel\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import NaiveBayesUtil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_dataloader\n",
    "# don't need cuda until using virtual machine\n",
    "from BERTModel import BERTGenreClassification, train_model\n",
    "from torch import manual_seed, cuda\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#autoreload ensures that we're always using the most current edits to our models\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "First we must perform a train-test split of our data. We choose an 80/20 split of our original cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping NaN values: (42661, 5)\n",
      "Shape after dropping NaN values: (42660, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data, drop na values, train test split\n",
    "descriptions = pd.read_csv(\"cleanedData.csv\")\n",
    "descriptions = descriptions.dropna()\n",
    "train, test = train_test_split(descriptions, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#more data cleaning \n",
    "train = train.drop(\"Unnamed: 0\", axis= 1)\n",
    "test = test.drop(\"Unnamed: 0\", axis= 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we needed to split both our training and testing data into X and Y's in order to train our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the training data into X and Y\n",
    "train_X = train['description']\n",
    "train_y1 = train['genre1']\n",
    "train_y2 = train['genre2']\n",
    "train_y3 = train['genre3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the testing data into X and Y\n",
    "test_X = test['description']\n",
    "test_y1 = test['genre1']\n",
    "test_y2 = test['genre2']\n",
    "test_y3 = test['genre3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes = NBLangIDModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the NB model \n",
    "naiveBayes.fit(train_X.tolist(), train_y1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the top three keys with highest porbability values in our prediction dictionary \n",
    "def argmaxThree(scoreDict: Dict[Any, float]) -> Any:\n",
    "    sortedScores = sorted(scoreDict.items(), key=lambda x: x[1], reverse=True)\n",
    "    topThreeKeys = [key for key, _ in sortedScores[:3]]\n",
    "    return topThreeKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using the NB model, obtaining the three genres with highest probability for each book entry using argmaxThree\n",
    "predictions = test_X.apply(lambda sentence: naiveBayes.predict_one_log_proba(sentence))\n",
    "argmaxPreds = [argmaxThree(dictionary) for dictionary in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Naive Bayes Model\n",
    "\n",
    "Next, we go into the evaluation phase for our Naive Bayes Model. We calculate multiple measures of accuracy, including position-dependent and position-independent accuracy, as well as looking at the percentage of time the model got at least k (where k= 1,2,3) genres correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position-dependent accuracy w/ breakdown by position (which genre1/2/3 placement was predicted most frequently accurately)\n",
    "def AccPosDepend(argmaxPreds, Y1, Y2, Y3):\n",
    "    totalRows = len(argmaxPreds)\n",
    "    correctPreds1 = 0\n",
    "    correctPreds2 = 0\n",
    "    correctPreds3 = 0\n",
    "\n",
    "    for pred, y1, y2, y3 in zip(argmaxPreds, Y1, Y2, Y3):\n",
    "        pred_y1, pred_y2, pred_y3 = pred\n",
    "        if pred_y1 == y1:\n",
    "            correctPreds1 += 1\n",
    "        if pred_y2 == y2:\n",
    "            correctPreds2 += 1\n",
    "        if pred_y3 == y3:\n",
    "            correctPreds3 += 1\n",
    "\n",
    "    correctPredsTotal = (correctPreds1 + correctPreds2 + correctPreds3 ) /3\n",
    "    genre1Acc = correctPreds1 / totalRows\n",
    "    genre2Acc = correctPreds2 / totalRows\n",
    "    genre3Acc = correctPreds3 / totalRows\n",
    "    accuracy = correctPredsTotal / totalRows\n",
    "    return accuracy, genre1Acc, genre2Acc, genre3Acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position independent accuracy, w/ breakdown by order in which NB model predicted label breakdown (so like which one the model thought was the greatest chance of being the true label)\n",
    "def AccPosNonDepend(argmaxPreds, Y1, Y2, Y3):\n",
    "    totalRows = len(argmaxPreds)\n",
    "    correctPreds1 = 0\n",
    "    correctPreds2 = 0\n",
    "    correctPreds3 = 0\n",
    "\n",
    "    for pred, y1, y2, y3 in zip(argmaxPreds, Y1, Y2, Y3):\n",
    "        pred_y1, pred_y2, pred_y3 = pred\n",
    "        if pred_y1 == y1 or pred_y1 == y2 or pred_y1 == y3:\n",
    "            correctPreds1 += 1\n",
    "        if pred_y2 == y1 or pred_y2 == y2 or pred_y2 == y3:\n",
    "            correctPreds2 += 1\n",
    "        if pred_y3 == y1 or pred_y3 == y2 or pred_y3 == y3:\n",
    "            correctPreds3 += 1\n",
    "\n",
    "    correctPredsTotal = (correctPreds1 + correctPreds2 + correctPreds3) /3\n",
    "    genre1Acc = correctPreds1 / totalRows\n",
    "    genre2Acc = correctPreds2 / totalRows\n",
    "    genre3Acc = correctPreds3 / totalRows\n",
    "    accuracy = correctPredsTotal / totalRows\n",
    "    return accuracy, genre1Acc, genre2Acc, genre3Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for how many you want to see correct, at least 1, at least 2, at least 3... \n",
    "#position independent\n",
    "def AccLevels(argmaxPreds, Y1, Y2, Y3, howManyCorrect):\n",
    "    totalRows = len(argmaxPreds)\n",
    "    correctPreds = 0  # Initialize counter for correct predictions\n",
    "\n",
    "    for pred, y1, y2, y3 in zip(argmaxPreds, Y1, Y2, Y3):\n",
    "        pred_y1, pred_y2, pred_y3 = pred\n",
    "        # Count how many predictions match exactly one of the labels\n",
    "        correct_labels = 0\n",
    "        if pred_y1 in [y1, y2, y3]:\n",
    "            correct_labels += 1\n",
    "        if pred_y2 in [y1, y2, y3]:\n",
    "            correct_labels += 1\n",
    "        if pred_y3 in [y1, y2, y3]:\n",
    "            correct_labels += 1\n",
    "        # Increment correctPreds if exactly one label is matched correctly for a prediction\n",
    "        if correct_labels >= howManyCorrect:\n",
    "            correctPreds += 1\n",
    "\n",
    "    accuracy = correctPreds / totalRows\n",
    "    genreAccs = [accuracy] * 3  # Since all genres share the same accuracy\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3156743241131427"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calcluating and saving accuracy measures \n",
    "accuracyPos, genre1Acc, genre2Acc, genre3Acc = AccPosDepend(argmaxPreds, test_y1, test_y2, test_y3)\n",
    "accuracyNonPos, pred1Acc, pred2Acc, pred3Acc = AccPosNonDepend(argmaxPreds, test_y1, test_y2, test_y3)\n",
    "accuracyLevels = AccLevels(argmaxPreds, test_y1, test_y2, test_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Custom BERT MultiClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#check what device we are using currently \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then tokenized our book summary descriptions using the AutoTokenizer available from the Hugging Face transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "label_vocab = naiveBayes.labels\n",
    "label_as_id = {l:k  for k, l in enumerate(label_vocab)}\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split our training data into two other data sets. One for evaluation and one for BERT training (80% and 20% of Naive Bayes training data, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train, bert_val = train_test_split(train, test_size= 0.2)\n",
    "#bert_train.drop(\"Unnamed: 0\", axis= 1)\n",
    "#bert_val.drop(\"Unnamed: 0\", axis= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For input into the BERT MultiTaskClassificationTrainer, we turn these data splits into a dictionary of Dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "raw_bert_train = Dataset.from_pandas(bert_train)\n",
    "raw_bert_val = Dataset.from_pandas(bert_val)\n",
    "\n",
    "ds = {'train': raw_bert_train, 'validation': raw_bert_val}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create id-label converters, so that we can keep track of the genre labels and their one-hot encoded forms. Using these converters, we use the `BERT_preprocess` function to tokenize the description feature of each data point and one-hot encode their genre labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 23985/23985 [00:21<00:00, 1135.63 examples/s]\n",
      "Map: 100%|██████████| 5997/5997 [00:05<00:00, 1174.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#latest test\n",
    "from util import BERT_preprocess\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "label_vocab = naiveBayes.labels\n",
    "id2label = {k:l  for k, l in enumerate(label_vocab)}\n",
    "label2id = {l:k  for k, l in enumerate(label_vocab)}\n",
    "\n",
    "for split in ds:\n",
    "    ds[split] = ds[split].map(lambda x: BERT_preprocess(x, id2label, tokenizer), remove_columns= ['description', 'genre1', 'genre2', 'genre3'])\n",
    "    #ds[split] = DataLoader(ds[split], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code chunk tests the `BERT_preprocess` function on a single data point to ensure that its output is what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2668, 10955, 2000, 2668, 1010, 10424, 10448, 1012, 1012, 1012, 2216, 2141, 2197, 2097, 2191, 1996, 2034, 1012, 1012, 1012, 2005, 25869, 6038, 2097, 2022, 20225, 2053, 2062, 1012, 2093, 2086, 2044, 1996, 8364, 2006, 11320, 8585, 2890, 2001, 4196, 1010, 10424, 10448, 2038, 2179, 2010, 2188, 1012, 1012, 1012, 2030, 2061, 2002, 7164, 1012, 1012, 1012, 16265, 8884, 2000, 1996, 3035, 1998, 9303, 17471, 2078, 1010, 10424, 10448, 2038, 2042, 4738, 5560, 1998, 8295, 2135, 2011, 1996, 3457, 10741, 2000, 4047, 1996, 2548, 2155, 1010, 1998, 2038, 4342, 2000, 2491, 2010, 4248, 12178, 1012, 2021, 2043, 2002, 2003, 2741, 2006, 1037, 28607, 3260, 2000, 1996, 2983, 1997, 25869, 6038, 1010, 2498, 2071, 2031, 4810, 2032, 2005, 2054, 2002, 4858, 1012, 2182, 2002, 11340, 1037, 5591, 2111, 2040, 2024, 2025, 2040, 2027, 4025, 1010, 1998, 2442, 4895, 22401, 2140, 2119, 1996, 2601, 9547, 1997, 27866, 1998, 1996, 15572, 1997, 1037, 2431, 1011, 5506, 4615, 1012, 1998, 1999, 2023, 20225, 1998, 8075, 2173, 1010, 2002, 2097, 7523, 2008, 2045, 2003, 1037, 2299, 5777, 1999, 2010, 2668, 1010, 1998, 2295, 10424, 10448, 2052, 2738, 2025, 1010, 1996, 2051, 2038, 2272, 2000, 4952, 1012, 13940, 1998, 6387, 1010, 3375, 1998, 26502, 8078, 1010, 10424, 10448, 1997, 1996, 27127, 2003, 1037, 28190, 8297, 2000, 9303, 17471, 2078, 1997, 1996, 2600, 1010, 2013, 1996, 7587, 2190, 1011, 4855, 1998, 4800, 1011, 2400, 1011, 3045, 3166, 1997, 2559, 2005, 4862, 23544, 2072, 1010, 7494, 15409, 1010, 2006, 1996, 15333, 6894, 16288, 2346, 1998, 1996, 11939, 1005, 1055, 2365, 1012, 102, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "import numpy as np\n",
    "BERT_preprocess({\n",
    "    'description': \"Blood sings to blood, Froi . . . Those born last will make the first . . . For Charyn will be barren no more. \\n Three years after the curse on Lumatere was lifted, Froi has found his home... Or so he believes...Fiercely loyal to the Queen and Finnikin, Froi has been trained roughly and lovingly by the Guard sworn to protect the royal family, and has learned to control his quick temper. But when he is sent on a secretive mission to the kingdom of Charyn, nothing could have prepared him for what he finds. Here he encounters a damaged people who are not who they seem, and must unravel both the dark bonds of kinship and the mysteries of a half-mad Princess.And in this barren and mysterious place, he will discover that there is a song sleeping in his blood, and though Froi would rather not, the time has come to listen.Gripping and intense, complex and richly imagined, Froi of the Exiles is a dazzling sequel to Finnikin of the Rock, from the internationally best-selling and multi-award-winning author of Looking for Alibrandi, Saving Francesca, On the Jellicoe Road and The Piper's Son.\",\n",
    "    'genre1': 'Fantasy',\n",
    "    'genre2': 'Young Adult',\n",
    "    'genre3': 'Romance'\n",
    "}, id2label, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define two functions. The first one transforms our logit output from the BERT model to probabilities, with which we can grab our top predictions. The second computes and returns our f1 accuracy metrics for the model to optimize upon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "def get_preds_from_logits(logits):\n",
    "    ret = torch.zeros(logits.shape)\n",
    "    probs = torch.nn.functional.softmax(logits, dim= 0)\n",
    "    \n",
    "    # for top 3\n",
    "    #_, idx = logits.topk(3, dim=0, largest= True)\n",
    "\n",
    "    # for top 1\n",
    "    _, idx = probs.topk(1, dim=0, largest= True)\n",
    "    ret[idx] = 1.0\n",
    "    return ret\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    final_metrics = {}\n",
    "\n",
    "    predictions = get_preds_from_logits(logits)\n",
    "\n",
    "    final_metrics['f1_micro'] = f1_score(labels, predictions, average= 'micro')\n",
    "    final_metrics['f1_macro'] = f1_score(labels, predictions, average= 'macro')\n",
    "\n",
    "    #Classification report\n",
    "    print('Classification report:')\n",
    "    print(classification_report(labels, predictions, zero_division = 0))\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the arguments for the MultiTaskClassificationTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define our MultiTaskClassificationTrainer, with which we train our BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AutoModelForSequenceClassification, Trainer, TrainerCallback, TrainingArguments\n",
    "\n",
    "class MultiTaskClassificationTrainer(Trainer):\n",
    "    def __init__(self, group_weights = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.group_weights = group_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        #loss = self.group_weights[0]*loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f'Epoch {state.epoch}: ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiating our BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', id2label=id2label, label2id=label2id).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the arguments for, and instantiating, our MultiTaskClassificationTrainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= './distil-fine-tuned',\n",
    "    learning_rate= LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = MultiTaskClassificationTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = ds['train'],\n",
    "    eval_dataset = ds['validation'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [PrinterCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(When on CPU) THIS WILL TAKE **3** HOURS TO TRAIN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 500/1500 [47:18<1:29:41,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6332, 'grad_norm': 13.228955268859863, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [1:32:27<44:54,  5.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9138, 'grad_norm': 14.959891319274902, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [2:17:37<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5415, 'grad_norm': 38.83592987060547, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "Epoch 1.0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'topk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:2298\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2302\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2660\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2662\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:3719\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3715\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3716\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3717\u001b[0m         )\n\u001b[0;32m   3718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3719\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m     11\u001b[0m logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[0;32m     12\u001b[0m final_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_preds_from_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m final_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_micro\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m f1_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m final_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m f1_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mget_preds_from_logits\u001b[1;34m(logits)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_preds_from_logits\u001b[39m(logits):\n\u001b[0;32m      5\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m     _, idx \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, largest\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     ret[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'topk'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
