{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do this install once, for experimenting with hugging face's transformers\n",
    "# CPU usage, just to ensure that model architecture will be working.\n",
    "# Will later likely need to use GPUs using a virtual environment,\n",
    "# IF: our dataset is too big, possibly not the case.\n",
    "#pip install transformers[torch]\n",
    "\n",
    "# Same here, run once\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports need necessary installs found above\n",
    "\n",
    "from naiveModel import NBLangIDModel\n",
    "from typing import Any, Dict, List, Optional, Tuple\n",
    "import NaiveBayesUtil\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import get_dataloader\n",
    "# don't need cuda until using virtual machine\n",
    "from BERTModel import BERTGenreClassification, train_model\n",
    "from torch import manual_seed, cuda\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "naiveBayes = NBLangIDModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping NaN values: (42661, 5)\n",
      "Shape after dropping NaN values: (42660, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data, train test split\n",
    "descriptions = pd.read_csv(\"cleanedData.csv\")\n",
    "\n",
    "print(\"Shape before dropping NaN values:\", descriptions.shape)\n",
    "descriptions = descriptions.dropna()\n",
    "print(\"Shape after dropping NaN values:\", descriptions.shape)\n",
    "\n",
    "#TESTING THIS\n",
    "# print(\"Rows to be dropped:\")\n",
    "# print(descriptions[descriptions['description'].isna() | ~descriptions['description'].apply(lambda x: isinstance(x, str))])\n",
    "# #drop the 511 rows where description column is NaN or not a string\n",
    "# descriptions = descriptions.dropna(subset=['description'])\n",
    "# descriptions = descriptions[descriptions['description'].apply(lambda x: isinstance(x, str))]\n",
    "\n",
    "train, test = train_test_split(descriptions, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(\"Unnamed: 0\", axis= 1)\n",
    "test = test.drop(\"Unnamed: 0\", axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train['description']\n",
    "train_y1 = train['genre1']\n",
    "train_y2 = train['genre2']\n",
    "train_y3 = train['genre3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test['description']\n",
    "test_y1 = test['genre1']\n",
    "test_y2 = test['genre2']\n",
    "test_y3 = test['genre3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the NB model \n",
    "naiveBayes.fit(train_X.tolist(), train_y1.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict using the NB model \n",
    "predictDict = naiveBayes.predict_one_log_proba(test_X.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmaxThree(score_dict: Dict[Any, float]) -> Any:\n",
    "    \"\"\"\n",
    "    Returns the key with the highest value in the dictionary\n",
    "\n",
    "    Args:\n",
    "        score_dict (Dict[Any, float]): the dictionary\n",
    "\n",
    "    Returns:\n",
    "        Any: the key with the highest value\n",
    "    \"\"\"\n",
    "    sorted_scores = sorted(score_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    topThreeKeys = [key for key, _ in sorted_scores[:3]]\n",
    "    return topThreeKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# position-dependent accuracy w/ breakdown by position (which genre1/2/3 placement was predicted most frequently accurately)\n",
    "def AccPosDepend(argmaxPreds, Y1, Y2, Y3):\n",
    "    totalRows = len(argmaxPreds)\n",
    "    correctPreds1 = 0\n",
    "    correctPreds2 = 0\n",
    "    correctPreds3 = 0\n",
    "\n",
    "    for pred, y1, y2, y3 in zip(argmaxPreds, Y1, Y2, Y3):\n",
    "        pred_y1, pred_y2, pred_y3 = pred\n",
    "        if pred_y1 == y1:\n",
    "            correctPreds1 += 1\n",
    "        if pred_y2 == y2:\n",
    "            correctPreds2 += 1\n",
    "        if pred_y3 == y3:\n",
    "            correctPreds3 += 1\n",
    "\n",
    "    correctPredsTotal = (correctPreds1 + correctPreds2 + correctPreds3 ) /3\n",
    "    genre1Acc = correctPreds1 / totalRows\n",
    "    genre2Acc = correctPreds2 / totalRows\n",
    "    genre3Acc = correctPreds3 / totalRows\n",
    "    accuracy = correctPredsTotal / totalRows\n",
    "    return accuracy, genre1Acc, genre2Acc, genre3Acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#position non-dependent accuracy, w/ breakdown by order in which NB model predicted label breakdown (so like which one the model thought was the greatest chance of being the true label)\n",
    "def AccPosNonDepend(argmaxPreds, Y1, Y2, Y3):\n",
    "    totalRows = len(argmaxPreds)\n",
    "    correctPreds1 = 0\n",
    "    correctPreds2 = 0\n",
    "    correctPreds3 = 0\n",
    "\n",
    "    for pred, y1, y2, y3 in zip(argmaxPreds, Y1, Y2, Y3):\n",
    "        pred_y1, pred_y2, pred_y3 = pred\n",
    "        if pred_y1 == y1 or pred_y1 == y2 or pred_y1 == y3:\n",
    "            correctPreds1 += 1\n",
    "        if pred_y2 == y1 or pred_y2 == y2 or pred_y2 == y3:\n",
    "            correctPreds2 += 1\n",
    "        if pred_y3 == y1 or pred_y3 == y2 or pred_y3 == y3:\n",
    "            correctPreds3 += 1\n",
    "\n",
    "    correctPredsTotal = (correctPreds1 + correctPreds2 + correctPreds3) /3\n",
    "    genre1Acc = correctPreds1 / totalRows\n",
    "    genre2Acc = correctPreds2 / totalRows\n",
    "    genre3Acc = correctPreds3 / totalRows\n",
    "    accuracy = correctPredsTotal / totalRows\n",
    "    return accuracy, genre1Acc, genre2Acc, genre3Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy for how many you want to see correct, at least 1, at least 2, at least 3... \n",
    "def AccLevels(argmaxPreds, Y1, Y2, Y3, howManyCorrect):\n",
    "    totalRows = len(argmaxPreds)\n",
    "    correctPreds = 0  # Initialize counter for correct predictions\n",
    "\n",
    "    for pred, y1, y2, y3 in zip(argmaxPreds, Y1, Y2, Y3):\n",
    "        pred_y1, pred_y2, pred_y3 = pred\n",
    "        # Count how many predictions match exactly one of the labels\n",
    "        correct_labels = 0\n",
    "        if pred_y1 in [y1, y2, y3]:\n",
    "            correct_labels += 1\n",
    "        if pred_y2 in [y1, y2, y3]:\n",
    "            correct_labels += 1\n",
    "        if pred_y3 in [y1, y2, y3]:\n",
    "            correct_labels += 1\n",
    "        # Increment correctPreds if exactly one label is matched correctly for a prediction\n",
    "        if correct_labels >= howManyCorrect:\n",
    "            correctPreds += 1\n",
    "\n",
    "    accuracy = correctPreds / totalRows\n",
    "    genreAccs = [accuracy] * 3  # Since all genres share the same accuracy\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_X.apply(lambda sentence: naiveBayes.predict_one_log_proba(sentence))\n",
    "argmaxPreds = [argmaxThree(dictionary) for dictionary in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3156743241131427"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracyPos, genre1Acc, genre2Acc, genre3Acc = AccPosDepend(argmaxPreds, test_y1, test_y2, test_y3)\n",
    "accuracyNonPos, pred1Acc, pred2Acc, pred3Acc = AccPosNonDepend(argmaxPreds, test_y1, test_y2, test_y3)\n",
    "accuracyLevels = AccLevels(argmaxPreds, test_y1, test_y2, test_y3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Custom BERT MultiClassificationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking away leading whitespace\n",
    "train['genre1'] = train['genre1'].str.strip()\n",
    "train['genre2'] = train['genre2'].str.strip()\n",
    "train['genre3'] = train['genre3'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "label_vocab = naiveBayes.labels\n",
    "\n",
    "label_as_id = {l:k  for k, l in enumerate(label_vocab)}\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_train, bert_val = train_test_split(train, test_size= 0.2)\n",
    "#bert_train.drop(\"Unnamed: 0\", axis= 1)\n",
    "#bert_val.drop(\"Unnamed: 0\", axis= 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "raw_bert_train = Dataset.from_pandas(bert_train)\n",
    "raw_bert_val = Dataset.from_pandas(bert_val)\n",
    "\n",
    "ds = {'train': raw_bert_train, 'validation': raw_bert_val}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 23985/23985 [00:21<00:00, 1135.63 examples/s]\n",
      "Map: 100%|██████████| 5997/5997 [00:05<00:00, 1174.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#latest test\n",
    "from util import BERT_preprocess\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "label_vocab = naiveBayes.labels\n",
    "id2label = {k:l  for k, l in enumerate(label_vocab)}\n",
    "label2id = {l:k  for k, l in enumerate(label_vocab)}\n",
    "\n",
    "for split in ds:\n",
    "    ds[split] = ds[split].map(lambda x: BERT_preprocess(x, id2label, tokenizer), remove_columns= ['description', 'genre1', 'genre2', 'genre3'])\n",
    "    #ds[split] = DataLoader(ds[split], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2668, 10955, 2000, 2668, 1010, 10424, 10448, 1012, 1012, 1012, 2216, 2141, 2197, 2097, 2191, 1996, 2034, 1012, 1012, 1012, 2005, 25869, 6038, 2097, 2022, 20225, 2053, 2062, 1012, 2093, 2086, 2044, 1996, 8364, 2006, 11320, 8585, 2890, 2001, 4196, 1010, 10424, 10448, 2038, 2179, 2010, 2188, 1012, 1012, 1012, 2030, 2061, 2002, 7164, 1012, 1012, 1012, 16265, 8884, 2000, 1996, 3035, 1998, 9303, 17471, 2078, 1010, 10424, 10448, 2038, 2042, 4738, 5560, 1998, 8295, 2135, 2011, 1996, 3457, 10741, 2000, 4047, 1996, 2548, 2155, 1010, 1998, 2038, 4342, 2000, 2491, 2010, 4248, 12178, 1012, 2021, 2043, 2002, 2003, 2741, 2006, 1037, 28607, 3260, 2000, 1996, 2983, 1997, 25869, 6038, 1010, 2498, 2071, 2031, 4810, 2032, 2005, 2054, 2002, 4858, 1012, 2182, 2002, 11340, 1037, 5591, 2111, 2040, 2024, 2025, 2040, 2027, 4025, 1010, 1998, 2442, 4895, 22401, 2140, 2119, 1996, 2601, 9547, 1997, 27866, 1998, 1996, 15572, 1997, 1037, 2431, 1011, 5506, 4615, 1012, 1998, 1999, 2023, 20225, 1998, 8075, 2173, 1010, 2002, 2097, 7523, 2008, 2045, 2003, 1037, 2299, 5777, 1999, 2010, 2668, 1010, 1998, 2295, 10424, 10448, 2052, 2738, 2025, 1010, 1996, 2051, 2038, 2272, 2000, 4952, 1012, 13940, 1998, 6387, 1010, 3375, 1998, 26502, 8078, 1010, 10424, 10448, 1997, 1996, 27127, 2003, 1037, 28190, 8297, 2000, 9303, 17471, 2078, 1997, 1996, 2600, 1010, 2013, 1996, 7587, 2190, 1011, 4855, 1998, 4800, 1011, 2400, 1011, 3045, 3166, 1997, 2559, 2005, 4862, 23544, 2072, 1010, 7494, 15409, 1010, 2006, 1996, 15333, 6894, 16288, 2346, 1998, 1996, 11939, 1005, 1055, 2365, 1012, 102, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'label': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "import numpy as np\n",
    "BERT_preprocess({\n",
    "    'description': \"Blood sings to blood, Froi . . . Those born last will make the first . . . For Charyn will be barren no more. \\n Three years after the curse on Lumatere was lifted, Froi has found his home... Or so he believes...Fiercely loyal to the Queen and Finnikin, Froi has been trained roughly and lovingly by the Guard sworn to protect the royal family, and has learned to control his quick temper. But when he is sent on a secretive mission to the kingdom of Charyn, nothing could have prepared him for what he finds. Here he encounters a damaged people who are not who they seem, and must unravel both the dark bonds of kinship and the mysteries of a half-mad Princess.And in this barren and mysterious place, he will discover that there is a song sleeping in his blood, and though Froi would rather not, the time has come to listen.Gripping and intense, complex and richly imagined, Froi of the Exiles is a dazzling sequel to Finnikin of the Rock, from the internationally best-selling and multi-award-winning author of Looking for Alibrandi, Saving Francesca, On the Jellicoe Road and The Piper's Son.\",\n",
    "    'genre1': 'Fantasy',\n",
    "    'genre2': 'Young Adult',\n",
    "    'genre3': 'Romance'\n",
    "}, id2label, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "\n",
    "def get_preds_from_logits(logits):\n",
    "    ret = torch.zeros(logits.shape)\n",
    "    _, idx = logits.topk(3, dim=0, largest= True)\n",
    "    ret[idx] = 1.0\n",
    "    return ret\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    final_metrics = {}\n",
    "\n",
    "    predictions = get_preds_from_logits(logits)\n",
    "\n",
    "    final_metrics['f1_micro'] = f1_score(labels, predictions, average= 'micro')\n",
    "    final_metrics['f1_macro'] = f1_score(labels, predictions, average= 'macro')\n",
    "\n",
    "    #Classification report\n",
    "    print('Classification report:')\n",
    "    print(classification_report(labels, predictions, zero_division = 0))\n",
    "\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-4\n",
    "MAX_LENGTH = 256\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import  AutoModelForSequenceClassification, Trainer, TrainerCallback, TrainingArguments\n",
    "\n",
    "class MultiTaskClassificationTrainer(Trainer):\n",
    "    def __init__(self, group_weights = None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.group_weights = group_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs = False):\n",
    "        labels = inputs.pop('labels')\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "\n",
    "        loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        #loss = self.group_weights[0]*loss\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_epoch_end(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f'Epoch {state.epoch}: ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', id2label=id2label, label2id=label2id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= './distil-fine-tuned',\n",
    "    learning_rate= LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    load_best_model_at_end=True,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = MultiTaskClassificationTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = ds['train'],\n",
    "    eval_dataset = ds['validation'],\n",
    "    compute_metrics = compute_metrics,\n",
    "    callbacks = [PrinterCallback]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THIS WILL TAKE **3** HOURS TO TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 500/1500 [47:18<1:29:41,  5.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6332, 'grad_norm': 13.228955268859863, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1000/1500 [1:32:27<44:54,  5.39s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.9138, 'grad_norm': 14.959891319274902, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [2:17:37<00:00,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.5415, 'grad_norm': 38.83592987060547, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "Epoch 1.0: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'topk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:2298\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2298\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2302\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:2662\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2660\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2661\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[1;32m-> 2662\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2663\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:3467\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3464\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3466\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3467\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3471\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3473\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3477\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\transformers\\trainer.py:3719\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3715\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3716\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3717\u001b[0m         )\n\u001b[0;32m   3718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3719\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3720\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3721\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(eval_pred)\u001b[0m\n\u001b[0;32m     11\u001b[0m logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[0;32m     12\u001b[0m final_metrics \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 14\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mget_preds_from_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m final_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_micro\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m f1_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmicro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m final_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m f1_score(labels, predictions, average\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mget_preds_from_logits\u001b[1;34m(logits)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_preds_from_logits\u001b[39m(logits):\n\u001b[0;32m      5\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 6\u001b[0m     _, idx \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m(\u001b[38;5;241m3\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, largest\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m     ret[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'topk'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything past this point is implementing HW5 BERT\n",
    "I kind of give up here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101, 101,\n",
       "         101, 101]),\n",
       " tensor([ 2668, 14015,  2129,  1996,  2641,  2182,  3393,  6832,  1999,  3243,\n",
       "          1999,  1000,  1996, 11810, 10881,  4098]),\n",
       " tensor([10955,  4832,  2079,  9957,  2028,  2003, 22294,  4454,  1996,  9200,\n",
       "          2023,  9049,  6622,  2038, 23075, 11338]),\n",
       " tensor([2000, 2012, 2017, 1004, 1997, 1996, 2063, 2001, 3500, 2028, 7893, 1010,\n",
       "         1997, 2196, 2038, 7847]),\n",
       " tensor([2668, 1037, 8081, 7015, 4701, 2034, 1040, 2019, 1997, 2851, 3117, 2601,\n",
       "         6792, 9505, 2467, 9257]),\n",
       " tensor([ 1010, 16760,  1037,  3319,  1005,  2047,  1005,  2248,  2687,  2003,\n",
       "          1010,  1010,  3640,  1996,  2908,  2015]),\n",
       " tensor([10424,  1012,  2293, 10359,  1055,  5449,  4300,  9575,  1010,  1996,\n",
       "          2990,  1998,  7070,  1000,  2005,  3268]),\n",
       " tensor([10448,  2016,  2008,  2178,  4602,  1997,  2003,  1010, 12849,  2338,\n",
       "          2003, 17950, 20062,  6825,  1996,  1037]),\n",
       " tensor([ 1012,  1998,  1521, 10501,  3983,  2412,  2019,  6037, 11319,  2008,\n",
       "          1037,  2517,  2046,  1997,  3308,  4251]),\n",
       " tensor([ 1012,  1996,  1055,  3396,  1011,  1011, 10990,  2006,  7842,  2081,\n",
       "          3671,  1000,  1996,  4676,  2828,  2166]),\n",
       " tensor([1012, 3588, 2042, 3453, 2301, 2759, 1010, 1996, 2912, 5696, 1010, 1006,\n",
       "         2287, 1000, 1997, 1999]),\n",
       " tensor([ 2216,  3626,  3714,  1010,  4898,  6172,  8687,  2047,  3211,  9566,\n",
       "          2779, 22358,  1011,  4237,  2158,  1996]),\n",
       " tensor([ 2141,  1997,  3458, 24020,  1010,  3213,  7613,  2259, 20709,  8029,\n",
       "          7032, 13259,  2214,  2013,  1012,  9435]),\n",
       " tensor([ 2197,  1996,  7192, 14017,  6819, 11044,  1997,  2335, 15210,  2890,\n",
       "          1011,  1007,  3160,  2236,  2044,  1997]),\n",
       " tensor([ 2097, 12098,  1029, 27922,  2140,  2310,  1996, 24304,  2000,  1037,\n",
       "          2095,  1010,  1997,  6825,  2016,  3190]),\n",
       " tensor([ 2191,  3995,  2005, 18595, 24546, 12119,  5722,  2862, 10930,  2732,\n",
       "          1011,  2023,  3251,  1012, 10070,  1010]),\n",
       " tensor([ 1996,  2462,  2086,  3215, 11240,  1005,  1997,  2005, 29312,  1999,\n",
       "          2214,  3117,  4895,  1996,  2013,  2127]),\n",
       " tensor([ 2034,  2071,  1010,  6038,  2121,  1055,  2332,  2058,  2863,  2010,\n",
       "          1012,  1997, 14821,  4310,  2028,  1996]),\n",
       " tensor([ 1012,  2709,  8869,  1010,  2290, 26162,  4300,  1037,  2167,  3128,\n",
       "          2002,  6860,  2094,  5197, 11704,  2154]),\n",
       " tensor([ 1012,  2188,  4202, 17377,  2580,  3117, 10760,  2095,  2690,  3725,\n",
       "          1521,  1998,  6792,  1997,  3276,  2002]),\n",
       " tensor([ 1012,  2007,  2699,  8418,  6382,  1997,  2279,  1998,  2082,  1010,\n",
       "          1055, 14583,  2003,  2010,  2000, 21811]),\n",
       " tensor([ 2005,  1996,  2000,  3070, 28626, 11508, 11552,  4855,  1012,  7411,\n",
       "         15896,  2013,  2412,  2147,  1996,  2015]),\n",
       " tensor([25869, 21880,  5293, 27685,  1998,  8993,  3179,  2062,  1999,  2010,\n",
       "          2007,  2019,  2825,  3658,  2279,  2588]),\n",
       " tensor([ 6038,  2112,  2055,  1010, 28802,  1999,  1999,  2084,  2465,  8200,\n",
       "          3348,  9586,  1012,  2738,  1010,  1037]),\n",
       " tensor([ 2097, 10222,  6066,  2859, 29376,  2058,  1996,  2274,  1010,  1010,\n",
       "          1010,  2400,  4235,  1999,  2014,  8075]),\n",
       " tensor([ 2022,  2891, 12621,  1005,  2000,  1037, 14161,  2454,  2002,  8040,\n",
       "          2998,  1011,  5868,  2010,  2197,  8730]),\n",
       " tensor([20225,  6231,  1012,  1055, 17279,  2301,  6799,  4809, 11791,  7875,\n",
       "          1010,  3045,  1998,  5456,  6898, 25213]),\n",
       " tensor([ 2053,  1998,  2002,  2877,  1996,  1012,  2121,  4969,  1037, 13288,\n",
       "          1998,  3166,  2172,  1998,  2435,  1012]),\n",
       " tensor([2062, 3046, 2001, 9974, 6569, 1999, 5092, 1012, 3168, 2135, 2010, 2003,\n",
       "         1011, 3949, 2014, 2116]),\n",
       " tensor([ 1012,  2000,  1996,  1998,  2015,  1996,  9102,  2085,  1997, 14742,\n",
       "         13523,  1037,  6936,  1997,  1037,  4326]),\n",
       " tensor([ 2093,  2644,  2028, 11170,  1998,  3117,  4438,  1010, 27880,  2806,\n",
       "         12228, 24560,  1010,  3412,  5256,  2111]),\n",
       " tensor([ 2086,  3409,  2307,  1010, 19817,  1010,  2186,  2320,  2004,  1998,\n",
       "          2303,  2075,  2023,  1010,  6279,  2024]),\n",
       " tensor([ 2044,  2431,  2293, 21109, 18655,  2034,  1010,  2153,  2002,  3338,\n",
       "          1012, 12185,  4438,  2030,  2655,  4699]),\n",
       " tensor([ 1996,  1011,  1997, 18534,  3111,  6775,  3393,  1010, 14444, 18278,\n",
       "          2002,  2046,  2338,  9280,  1999,  1999]),\n",
       " tensor([ 8364,  2668,  2014,  4751,  1997, 17137, 22294,  3817,  2008,  1010,\n",
       "          1521,  1996, 15102,  3412,  1996,  4098]),\n",
       " tensor([2006, 1998, 2166, 2007, 3679, 5280, 2063, 2175, 1996, 3109, 1055, 2601,\n",
       "         2129, 1010, 5409, 1998]),\n",
       " tensor([11320,  3409,  1010,  7214,  2166,  8267,  1040, 16930,  2111,  1011,\n",
       "          2288,  2540,  6792,  5876,  2825,  2010]),\n",
       " tensor([ 8585, 13035,  1998,  5461,  2005,  1037,  1005,  2319,  2105,  2005,\n",
       "          1037,  1997,  2064,  1999,  2126, 25213]),\n",
       " tensor([ 2890,  2013,  2043,  2000,  2220,  8075,  4300,  2038,  2032,  1011,\n",
       "          2843,  1996, 12636,  2010,  1012,  1012]),\n",
       " tensor([2001, 2183, 2002, 3443, 4467, 3661, 2003, 2517, 2552, 5898, 2006, 2035,\n",
       "         1999, 4812, 2007, 2010]),\n",
       " tensor([ 4196,  2000,  3478, 27427, 13200,  4851, 14477,  1037,  2066,  7984,\n",
       "          2010,  1011,  1037,  2046,  2014,  5456]),\n",
       " tensor([ 1010,  2162,  2000, 20806,  1999,  2032,  6374, 23222,  2027, 15732,\n",
       "          2568,  2137,  2088,  1996,  2088,  5260]),\n",
       " tensor([10424,  1012,  2709,  3468,  2637,  2000,  2094, 10752,  1005,  1012,\n",
       "          1010,  2611,  1997,  9787, 10909,  2032]),\n",
       " tensor([10448,  2030,  2014,  9668,  1012,  9570,  1998,  1997,  2128,  1996,\n",
       "          2164,  1012,  2969,  2004,  1010,  2000]),\n",
       " tensor([ 2038,  2027,  2293,  1997,  2010,  1037,  3143,  1996,  3788,  3117,\n",
       "          2010,  5587,  1011,  1037,  2016, 14596]),\n",
       " tensor([ 2179,  2071,  1010,  3679, 10862, 11013,  1012,  6745,  2006,  2008,\n",
       "          2388,  2100,  6224,  2878,  1521,  2914]),\n",
       " tensor([ 2010,  3613,  1037,  2166, 11633, 23869,  2761,  9556,  6763,  2180,\n",
       "          1521,  7658, 13059,  1998,  1055,  1010]),\n",
       " tensor([ 2188,  2037,  2112,  2104, 15921,  1999,  2405,  1999, 18223,  1996,\n",
       "          1055,  7811,  5130,  1999, 10741,  1037]),\n",
       " tensor([1012, 8795, 1997, 1037, 1997, 6220, 1999, 7366, 2015, 7725, 2331, 2038,\n",
       "         1517, 2010, 2125, 3595]),\n",
       " tensor([ 1012,  2000,  2014,  8401,  2122,  1012, 16459,  1998,  1010,  2034,\n",
       "          1012,  2467,  3251,  2236,  2273,  2082]),\n",
       " tensor([ 1012,  2424,  2351,  1010, 15716,  2004,  2629,  4167,  1998,  2668,\n",
       "          2009,  2042,  3565, 17261,  1517,  2073]),\n",
       " tensor([ 2030,  1996,  5091,  2139,  2111,  2002,  2011,  2671,  2493,  2400,\n",
       "          1521,  7014, 11452,  3218,  2926,  2307]),\n",
       " tensor([ 2061,  2160,  1012, 28600,  1005,  4641,  2520,  1010,  1998,  2005,\n",
       "          1055, 13737,  2015,  1012,  2216,  2477]),\n",
       " tensor([ 2002,  1997,  2030,  7088,  1055,  2041,  6187,  8669,  5089,  1996,\n",
       "          2042,  1005,  1010,  1999,  1997, 26751]),\n",
       " tensor([ 7164, 23003,  2061,  6774,  3268,  2005, 14226,  2008, 11455,  2190,\n",
       "          2698,  1055,  5661,  3437,  1996,  2032]),\n",
       " tensor([ 1012,  1010,  2016,  2576,  2003, 20154,  1010,  2057,  4025,  2834,\n",
       "          2086,  2190,  1010,  2000,  2919,  1012]),\n",
       " tensor([ 1012,  2073,  2245,  6939,  1037,  3016,  2909,  2024, 10363,  4126,\n",
       "          1010,  2767,  2030,  3105,  1011,  2021]),\n",
       " tensor([1012, 2027, 1012, 1012, 2350, 1998, 2726, 1523, 1012, 3117, 2021, 1998,\n",
       "         3633, 1010, 2879, 2601]),\n",
       " tensor([16265,  2453,  2085,  1999,  3997,  1996, 15451, 17502,  2004,  1999,\n",
       "          2990,  9480,  1517,  2034,  3528,  2477]),\n",
       " tensor([ 8884,  2022,  2016,  2028,  1997, 10162, 10253,  2000,  1037,  1996,\n",
       "          2038,  3812,  2043,  2405,  1012,  2024]),\n",
       " tensor([ 2000,  2583,  1998,  2158,  1996, 24239,  1005,  7532,  4677,  2142,\n",
       "          2664,  1012,  2045,  1999,  5624,  3403]),\n",
       " tensor([ 1996,  2000,  6066,  1005, 12495,  1010,  1055,  1524,  1997,  2983,\n",
       "          2000,  7014,  2003, 10204,  5232,  1010]),\n",
       " tensor([ 3035,  2330,  2024,  1055, 18980,  1037,  3393,  1998, 23512,  2003,\n",
       "          2292,  4455,  2053,  1999,  2003,  2205]),\n",
       " tensor([ 1998,  1996,  6631,  6331,  6002,  3626, 22294,  1996,  6677,  2085,\n",
       "          2014,  1996,  2430,  3999,  3599,  1012]),\n",
       " tensor([ 9303,  4303,  1037,  1010,  1012,  2386,  2063, 10889,  4088,  2800,\n",
       "          2175,  7171,  3691,  1010,  1996,  2043]),\n",
       " tensor([17471,  1997,  5934, 17377, 11240,  2633,  1040,  2784,  2000,  1999,\n",
       "          1012,  1998,  2000, 11810,  4066,  4098]),\n",
       " tensor([ 2078,  2331,  2754,  3957,  2121,  7657,  1005,  4254,  4895,  2637,\n",
       "          1012,  5587,  2610, 13495,  2008, 10229]),\n",
       " tensor([ 1010,  1010,  1010,  2149,  2290,  2370,  4300,  1997, 10371,  2005,\n",
       "          1012,  2100,  2037,  1996, 10881,  2008]),\n",
       " tensor([10424,  5343,  1998,  1037,  1005,  2004,  3464,  2256,  2105,  1996,\n",
       "          2008,  7883,  4506,  5220,  4122,  3976]),\n",
       " tensor([10448,  2037,  2002, 13769,  1055,  2952,  1996,  6550,  2032,  2034,\n",
       "          2003,  2068,  1012,  2653,  2000,  3238]),\n",
       " tensor([ 2038,  2814,  1521, 13804,  4866,  2198,  2087,  2006,  1010,  2051,\n",
       "          1010,  2041,  1996,  1997,  4468, 22000]),\n",
       " tensor([ 2042, 11312,  1055,  2006,  2470,  6045, 10990,  2296,  2002,  1010,\n",
       "          2127,  1010,  3291,  9208,  1517,  1998]),\n",
       " tensor([ 4738,  1998,  4340,  1037,  1999, 14621,  1998,  7814,  3310,  1998,\n",
       "          2002,  1037,  1997, 15152,  9882, 12785]),\n",
       " tensor([5560, 4698, 2000, 2166, 1996, 2015, 8687, 1997, 2000, 5021, 6010, 2146,\n",
       "         6792, 1012, 1010, 2336]),\n",
       " tensor([ 1998, 20915,  2663,  4417,  4981,  1010,  7613,  2256,  7523,  4126,\n",
       "          4698,  1011,  2003,  2107, 26464,  2024]),\n",
       " tensor([ 8295,  2232,  2014,  2011,  1997,  1998,  1997,  3268,  2008,  3015,\n",
       "          8671,  2511,  2430,  3408,  1010, 14489]),\n",
       " tensor([ 2135,  2013,  2067,  3167,  4467,  2010,  1996,  1012,  2002,  2006,\n",
       "          1012,  2344,  2000,  2004, 11951,  1010]),\n",
       " tensor([ 2011, 16985,  1012,  1998, 12495, 17418,  5722,  2521,  2038,  2023,\n",
       "          4698,  1997,  2116,  1000,  1010,  2002]),\n",
       " tensor([ 1996,  7559,  6815,  2576, 27444,  1011,  1997,  2062,  2042,  2217,\n",
       "          8671,  2477,  2367,  2643,  1998,  4858]),\n",
       " tensor([ 3457,  2271,  2000, 12603,  1999,  1011,  2332,  2084,  2872,  1997,\n",
       "          2003,  2008,  4249,  1010, 24995,  2370]),\n",
       " tensor([10741,  1010,  2022,  1012, 22796,  2000,  4300,  2057,  1999,  1996,\n",
       "          3376,  2038,  1012,  1000,  1012,  1999]),\n",
       " tensor([2000, 1998, 1037, 1996, 6407, 2131, 1998, 2024, 1996, 4448, 1010, 2716,\n",
       "         2728, 1000, 2021, 1996]),\n",
       " tensor([ 4047,  4652,  2904,  2171,  1010,  2000,  1996, 24447, 10678,  2089,\n",
       "          6047,  2068, 18586,  9866,  2010,  2892]),\n",
       " tensor([ 1996,  9219,  2158,  3238,  2164,  1996,  7307,  5204,  2465,  2196,\n",
       "          1010,  2000, 14127,  1010,  5931, 10273]),\n",
       " tensor([ 2548,  2013,  1010, 11185,  1996,  2167,  1997,  1010,  1017,  2022,\n",
       "          1998,  1996, 23412,  1000,  8829,  1997]),\n",
       " tensor([ 2155,  2108,  2002,  1997,  5135,  6536,  1996,  2256,  1999,  1996,\n",
       "          2016, 26007,  1996,  1998, 17382,  2019]),\n",
       " tensor([ 1010, 27788,  1521,  1996,  3439,  1012,  2461,  3679,  2029,  2168,\n",
       "          4152,  1997,  3297,  1000,  1037,  3418]),\n",
       " tensor([ 1998, 10010,  1055,  3117,  2554,  2044,  2795, 11340,  1996,  1012,\n",
       "          2032,  2037,  3274,  4763,  2204,  5998]),\n",
       " tensor([ 2038, 23854,  2633,  1011,  1010, 13417,  1012,  2007,  3076,  3243,\n",
       "          1012,  2152,  8504,  1000,  2158,  2090]),\n",
       " tensor([ 4342,  1999,  2583,  1011,  9124, 10439,  2005,  3008,  2303,  9200,\n",
       "          3402,  1011,  1999,  2024,  2040,  2204]),\n",
       " tensor([2000, 1996, 2000, 2029, 2032, 8095, 4300, 1010, 2132, 2028, 2293, 2082,\n",
       "         2029, 1996, 1521, 1998]),\n",
       " tensor([ 2491,  9801,  2360,  4269,  2000,  2075,  2937, 18591,  4175,  2851,\n",
       "          3084, 10922,  1996,  3543,  1055,  4763]),\n",
       " tensor([ 2010,  2088,  2035,  1999, 13265,  3147,  4599,  2015,  2003, 13999,\n",
       "          3168,  1012,  1523, 29423,  3201,  1012]),\n",
       " tensor([ 4248,  1012,  1996,  3824,  2116,  1998,  4969,  1010,  2467,  9566,\n",
       "          1010,  2085, 10791,  1997,  2000,  2000]),\n",
       " tensor([12178, 29221,  2477,  4291,  4751,  9012,  1010, 23029,  2028,  8029,\n",
       "          1998,  2027,  1524,  2010,  2424,  5788]),\n",
       " tensor([1012, 2346, 2016, 4290, 1997, 1010, 2023, 1010, 2062, 2890, 1996, 1005,\n",
       "         2565, 6685, 1523, 1010]),\n",
       " tensor([ 2021,  2027,  2734,  1011,  7156,  1996, 14726,  1998,  2084,  1005,\n",
       "          2925,  2128, 14841,  1012,  1996,  2002]),\n",
       " tensor([ 2043,  5630,  2000,  1011,  2166,  2952,  5592,  2130,  3517,  1055,\n",
       "          3849, 15995,  2102,  1998,  2028,  1005]),\n",
       " tensor([ 2002,  2000,  2963,  2003,  1012, 10313,  3179, 12358,  1012,  8085,\n",
       "         17772,  2040,  2005,  2664,  1012,  2222]),\n",
       " tensor([ 2003,  2202,  2086,  4415,  2034,  2015,  2038,  4338,  2465, 10191,\n",
       "          1012,  3627, 11937,  1010,  1524,  2031]),\n",
       " tensor([ 2741,  1010,  3283, 17377,  2405,  2408,  1037,  2256,  1017,  1010,\n",
       "          2761,  1996,  2102,  3437,  2002,  2000]),\n",
       " tensor([ 2006,  2027,  1010,  2370,  2090,  1996,  8416, 14332,  2003,  1996,\n",
       "          2405, 20531,  2680,  2000,  1521, 11160]),\n",
       " tensor([ 1037,  2031,  2021,  1012,  4085,  7708,  8031,  1998, 11171,  2524,\n",
       "          2004,  6975,  2049,  3105,  1055,  2006]),\n",
       " tensor([28607,  2000,  2064,  1999,  1998,  5949,  1010,  7461,  2011,  1011,\n",
       "          2048, 15138, 14726,  1010,  4340,  1037]),\n",
       " tensor([ 3260,  9241,  2016,  1996,  3851,  2015, 10557,  4442,  1037,  2283,\n",
       "          3584,  4686,  9248,  3383,  2000,  2897]),\n",
       " tensor([ 2000,  1010,  2903, 10305,  1999,  1999, 12115,  2802,  2310,  2075,\n",
       "          6002,  1010,  1010, 11810,  4139,  1997]),\n",
       " tensor([ 1996,  2138,  2032, 10530,  4467,  3945,  1010,  2256, 15465,  1010,\n",
       "          1010,  8615,  7607,  1005, 10881,  6074]),\n",
       " tensor([2983, 2051, 1029, 1997, 1010, 1997, 1998, 4230, 3993, 7968, 8141, 1998,\n",
       "         2049, 1055, 2013, 1998]),\n",
       " tensor([ 1997,  2003,  2054,  1037,  2122,  4762,  2003,  1517,  4382, 26775,\n",
       "          2097,  2628,  4646,  2087,  2014, 17477]),\n",
       " tensor([25869,  2770,  3084,  4424,  2176,  1012, 21972,  2091,  3625,  8684,\n",
       "          2085,  2011,  2000,  6801,  2969,  2015]),\n",
       " tensor([ 6038,  2041,  2023,  8087,  2808,  4704, 15981,  2000,  2005,  2075,\n",
       "          2031,  1996,  1037,  2147,  1011,  1010]),\n",
       " tensor([ 1010,  1012,  2051,  1010,  2020,  2011,  1999,  1996, 24665, 15025,\n",
       "          1996,  2060,  5041,  1010,  9770,  1996]),\n",
       " tensor([ 2498, 11721,  2367, 17377,  2641,  2087,  2019,  2504, 15808,  4988,\n",
       "          4495,  3057,  8674,  2003, 22560, 11067]),\n",
       " tensor([ 2071,  5243,  2013,  7065,  1037,  1997, 11552,  1997,  8462,  2990,\n",
       "          2000,  1517,  1997,  2025,  1998,  1997]),\n",
       " tensor([ 2031,  1010,  2035, 17417,  2309,  2010,  7540,  2256,  6677, 11968,\n",
       "          3582,  2127,  5739,  2019,  2663,  2010]),\n",
       " tensor([ 4810,  1996,  2010,  3215,  2147,  3626, 18382,  9165,  1999, 20470,\n",
       "          2990,  1996,  1010,  9491,  2014, 18328]),\n",
       " tensor([2032, 2668, 2060, 1996, 2011, 1010, 1012, 1517, 2019, 7231, 1998, 2402,\n",
       "         1998, 1999, 2540, 1010]),\n",
       " tensor([ 2005, 15222,  3714,  2430, 11240,  1998,  3794,  2005,  3947,  1010,\n",
       "          4698,  2047,  6083,  8006,  1012,  1998]),\n",
       " tensor([ 2054, 12096, 10659,  5312,  2121,  5642,  1037,  2204,  2000,  2040,\n",
       "          8671,  2873,  2129,  2004,  2004,  1996]),\n",
       " tensor([ 2002,  2100,  1029,  1997,  2290,  2011,  2047,  2030, 13225,  2003,\n",
       "          1521,  8480,  8141,  2172, 10881, 17115]),\n",
       " tensor([4858, 3011, 6066, 2010, 1010, 1037, 4955, 5665, 2049, 2025, 1055, 1012,\n",
       "         2064, 2004, 7480, 2373]),\n",
       " tensor([ 1012,  2388,  4282,  2166,  2040,  6538,  1998,  1012,  8741,  4452,\n",
       "          2466,  4658,  2119,  2009,  2039, 16936]),\n",
       " tensor([2182, 1010, 2002, 1010, 3832, 2137, 1996, 1999, 1012, 2000, 2013, 1998,\n",
       "         6611, 2003, 1999, 2306]),\n",
       " tensor([ 2002,  2038,  2064,  7118,  2008, 10566, 11552,  2591,  2000,  8815,\n",
       "          2927,  7991, 10791,  2019,  1996,  2032]),\n",
       " tensor([11340,  2275,  1521,  1010,  2027,  1010, 11249,  4454,  2644,  1996,\n",
       "          2000,  1010,  6481,  7749,  2227,  1012]),\n",
       " tensor([1037, 1996, 1056, 1999, 2022, 6045, 2011, 1010, 1996, 4277, 2203, 2019,\n",
       "         2000, 1997, 1997,  102]),\n",
       " tensor([ 5591,  3058,  2689,  3638,  3191, 14621, 18961,  3817, 13925,  1997,\n",
       "          1999, 12495,  2037,  1996,  5624,     0]),\n",
       " tensor([ 2111,  1997,  2037,  1010,  2004,  2015, 10154,  2175,  5402,  1996,\n",
       "          2028, 25556,  2219, 12613,  1521,     0]),\n",
       " tensor([ 2040,  2257, 10722,  2000,  4516,  4247,  8002, 16930, 13940,  2455,\n",
       "          3143,  2100,  3268,  2535,  1055,     0]),\n",
       " tensor([ 2024,  1015, 12274,  1996,  6002,  2010,  1006,  2319,  2010,  1006,\n",
       "          7427,  2013,  1998,  2008,  6706,     0]),\n",
       " tensor([ 2025,  2005,  7096,  7211,  1012,  4990,  7572, 15102,  2047,  2030,\n",
       "          1012,  1996,  6570,  9208, 13347,     0]),\n",
       " tensor([ 2040,  2014,  8918,  1997,  2122,  2000,  2050,  2019,  2082,  2130,\n",
       "           102,  4639, 10791,  8474,  1010,     0]),\n",
       " tensor([2027, 4125, 2627, 2010, 6572, 1996, 1029, 8361, 1010, 1996,    0, 2088,\n",
       "         6481, 2377, 1996,    0]),\n",
       " tensor([ 4025,  2000,  1010,  5593,  5383,  6536,  1000,  2047, 12849,  4277,\n",
       "             0,  2074,  2000,  1999,  6896,     0]),\n",
       " tensor([ 1010,  2373,  2021,  1010, 25795,  1010,  6068,  2671, 11319,  1997,\n",
       "             0,  3458,  2500,  1037,  2090,     0]),\n",
       " tensor([1998, 1012, 2065, 1037, 2517, 8087, 1007, 2007, 7288, 8992,    0, 2037,\n",
       "         1012, 2711, 2068,    0]),\n",
       " tensor([ 2442,  4698,  2002,  5593,  2011,  2075,  1010, 19828,  2000,  1007,\n",
       "             0,  3362,   102,  1005, 16270,     0]),\n",
       " tensor([ 4895, 20915,  1521, 21985,  5074, 10866,  2023, 13494,  2131,  2000,\n",
       "             0,  1010,     0,  1055,  7616,     0]),\n",
       " tensor([22401,  2232,  1055,  2012, 11338,  2566,  3872,  2005,  2000,  2131,\n",
       "             0,  2873,     0, 12663,  2000,     0]),\n",
       " tensor([ 2140,  1998,  2183,  1996,  2243, 12146,  1997,  2256,  1996,  2000,\n",
       "             0,  5624,     0,  2166, 17162,     0]),\n",
       " tensor([ 2119, 11312,  2000,  2287, 15864,  1998,  3393,  6970,  3953,  1996,\n",
       "             0,  4674,     0,  1012,  3798,     0]),\n",
       " tensor([ 1996,  2024,  2031,  1997,  1010,  7357, 22294, 28823,  1997,  3606,\n",
       "             0,  2413,     0,   102,  1012,     0]),\n",
       " tensor([ 2601, 13394,  2151,  2702, 13430,  2247,  2063,  2088,  1996,  1012,\n",
       "             0,  9891,     0,     0,  2021,     0]),\n",
       " tensor([ 9547,  1012,  3382,  2011,  2271,  1996,  1040,  1012,  8364, 11968,\n",
       "             0,  5587,     0,     0,  2043,     0]),\n",
       " tensor([ 1997,  2129,  1997,  2010, 28564,  2126,  1005,  2049,  1010, 20470,\n",
       "             0,  2100,     0,     0, 10881,     0]),\n",
       " tensor([27866,  2097,  2108,  2388,  2271,  1012,  4300,  2087,  2021,  7231,\n",
       "             0,  1998,     0,     0,  1521,     0]),\n",
       " tensor([1998, 1996, 2007, 1005, 2267, 2023, 2003, 8050, 2003, 2003,    0, 1996,\n",
       "            0,    0, 1055,    0]),\n",
       " tensor([1996, 2048, 1996, 1055, 1010, 2047, 2019, 5456, 2002, 8329,    0, 2060,\n",
       "            0,    0, 2627,    0]),\n",
       " tensor([15572,  1997,  2450, 17128,  1998,  1998, 27427,  1024,  4810,  1037,\n",
       "             0, 15138,     0,     0,  3310,     0]),\n",
       " tensor([ 1997,  2068,  2002, 14759,  9239, 14477,  2483,  2057,  2005, 23512,\n",
       "             0, 19000,     0,     0,  2067,     0]),\n",
       " tensor([ 1037,  2191,  7459,  1012, 11240,  6374, 11837,  2024,  1996,  6865,\n",
       "             0,  2015,     0,     0,  2000,     0]),\n",
       " tensor([ 2431,  2009,  1010,  2013,  2121,  2094, 19307,  2881,  5469,  7840,\n",
       "             0,  2046,     0,     0, 24542,     0]),\n",
       " tensor([ 1011,  2083,  2002, 16199,  2290,  5449,  4438,  2005,  2008,  2043,\n",
       "             0,  2014,     0,     0,  2014,     0]),\n",
       " tensor([ 5506, 16985,  1521, 12070,  1005,  1997,  2005, 27084,  3658,  2002,\n",
       "             0,  2166,     0,     0,  1010,     0]),\n",
       " tensor([ 4615,  7559,  2222,  5312,  1055,  1996,  2296,  2401,  3805, 21811,\n",
       "             0,  1012,     0,     0,  2014,     0]),\n",
       " tensor([ 1012,  2271,  2342,  2066, 24751,  2034,  2188,  8553,  1012,  2015,\n",
       "             0,  2069,     0,     0, 10069,     0]),\n",
       " tensor([1998, 1029, 2000, 2023, 2025, 1997, 3075, 1010, 1012, 2408,    0, 7014,\n",
       "            0,    0, 2089,    0]),\n",
       " tensor([ 1999, 18025,  8054,  1010,  2443,  2310,  1012,  7887,  1012,  1996,\n",
       "             0,  1010,     0,     0,  6033,     0]),\n",
       " tensor([ 2023,  1010,  8869, 17377,  1999, 12119,   102,  5117,  1029, 11547,\n",
       "             0,  4895,     0,     0,  1996,     0]),\n",
       " tensor([20225, 24907,  2008,  1005,  3041,  1005,     0,  1999,   102,  1997,\n",
       "             0, 21678,     0,     0,  3404,     0]),\n",
       " tensor([1998, 1010, 2014, 1055, 2394, 1055,    0, 1037,    0, 1996,    0, 3709,\n",
       "            0,    0, 5624,    0]),\n",
       " tensor([ 8075,  1998,  2925,  3638,  6572,  9313,     0,  1523,     0, 16596,\n",
       "             0,  2011,     0,     0,  2038,     0]),\n",
       " tensor([ 2173,  1999,  7460,  8483,  1012, 19147,     0, 15756,     0,  2239,\n",
       "             0,  1996,     0,     0,  2328,     0]),\n",
       " tensor([1010, 3255, 2007, 2408, 2338, 2186,    0, 7250,    0, 1997,    0, 2047,\n",
       "            0,    0, 2090,    0]),\n",
       " tensor([2002, 1010, 2032, 2051, 1015, 8235,    0, 1524,    0, 1037,    0, 6939,\n",
       "            0,    0, 2068,    0]),\n",
       " tensor([ 2097,  2027,  1012,  1998, 13999,  2135,     0,  2008,     0,  7272,\n",
       "             0,  1010,     0,     0,  1517,     0]),\n",
       " tensor([ 7523,  2024,  2123,  2686,  6382, 16636,     0,  8539,     0,  5928,\n",
       "             0,  3464,     0,     0,  1998,     0]),\n",
       " tensor([ 2008,  4510,  1521,  1010, 28626,  2015,     0,  2149,     0,  2966,\n",
       "             0,  2648,     0,     0,  7697,     0]),\n",
       " tensor([2045, 2583, 1056, 6360, 1998, 1996,    0, 4167,    0, 2155,    0, 2873,\n",
       "            0,    0, 2068,    0]),\n",
       " tensor([ 2003,  2000,  3335, 27816, 28802,  3117,     0,  2000,     0,  1012,\n",
       "             0,  1005,     0,     0,  4237,     0]),\n",
       " tensor([ 1037, 21811,  2023,  1996, 29376,  1005,     0,  4167,     0,  4340,\n",
       "             0,  1055,     0,     0,  2005,     0]),\n",
       " tensor([ 2299,  2006, 14726,  3267,  1010,  1055,     0,  2007,     0,  2000,\n",
       "             0,  3585,     0,     0,  2204,     0]),\n",
       " tensor([ 5777,  1999,  7091,  1997,  2037, 28322,     0,  2216,     0,  2131,\n",
       "             0,  4418,     0,     0,  1529,     0]),\n",
       " tensor([1999, 1996, 2000, 2166, 2093, 6888,    0, 2105,    0, 2000,    0, 1010,\n",
       "            0,    0,  102,    0]),\n",
       " tensor([ 2010,  2601,  1996,  2077,  2402,  1998,     0,  2149,     0,  1996,\n",
       "             0, 11333,     0,     0,     0,     0]),\n",
       " tensor([2668, 1998, 4895, 1010, 2336, 7224,    0, 1012,    0, 3953,    0, 4726,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 1010, 22641, 29278,  2076,  1010,  1012,     0,  2256,     0,  1997,\n",
       "             0,  1037,     0,     0,     0,     0]),\n",
       " tensor([ 1998,  5957, 18150,  1010,  1998,  2023,     0,  9597,     0,  1996,\n",
       "             0, 11259,     0,     0,     0,     0]),\n",
       " tensor([ 2295,  2008, 10880,  1998,  5408,  3179,     0,  2000,     0,  4028,\n",
       "             0,  2021,     0,     0,     0,     0]),\n",
       " tensor([10424,  4324,  2293,  2044,  2500,  2036,     0,  2500,     0,  2370,\n",
       "             0, 13925,     0,     0,     0,     0]),\n",
       " tensor([10448,  2047,  2466,  2859,  2040,  2950,     0,  1010,     0,  1010,\n",
       "             0,  3049,     0,     0,     0,     0]),\n",
       " tensor([ 2052, 22812,  2008,  1005,  2191,  1996,     0,  1998,     0,  2002,\n",
       "             0,  2000,     0,     0,     0,     0]),\n",
       " tensor([ 2738,  2012, 14408,  1055,  2039,  2434,     0, 17156,     0,  2855,\n",
       "             0, 12452,     0,     0,     0,     0]),\n",
       " tensor([ 2025,  2296, 21967, 16775,  1037,  1010,     0,  2000,     0,  4150,\n",
       "             0,  2014,     0,     0,     0,     0]),\n",
       " tensor([ 1010,  2735,  2058,  3451, 24501,  8292,     0,  2149,     0,  4372,\n",
       "             0,  2597,     0,     0,     0,     0]),\n",
       " tensor([ 1996,  1012,  2048,  4329,  4747, 29577,     0,  1010,     0,  7834,\n",
       "             0,  2004,     0,     0,     0,     0]),\n",
       " tensor([ 2051,  2027,  2454,  1012, 10421,  2098,     0,  2031,     0,  9072,\n",
       "             0,  1000,     0,     0,     0,     0]),\n",
       " tensor([ 2038,  2031,  4599, 17377,  2283,  4566,     0,  1037,     0,  1999,\n",
       "             0,  2327,     0,     0,     0,     0]),\n",
       " tensor([2272, 2053, 3784, 8418, 1997, 1010,    0, 2521,    0, 1037,    0, 2611,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 2000,  2126,  1012,  3070, 25430,  1998,     0,  1011,     0,  3748,\n",
       "             0,  1000,     0,     0,     0,     0]),\n",
       " tensor([ 4952,  1997,   102, 27685, 18352, 17160,     0,  4285,     0,  6172,\n",
       "             0,  1517,     0,     0,     0,     0]),\n",
       " tensor([ 1012, 26339,     0,  2003, 14070,  4751,     0,  6897,     0,  2008,\n",
       "             0,  2119,     0,     0,     0,     0]),\n",
       " tensor([13940,  1996,     0,  2028,  1996,  2055,     0,  4254,     0,  2097,\n",
       "             0,  2007,     0,     0,     0,     0]),\n",
       " tensor([1998, 4303,    0, 1997, 5635, 1996,    0, 1010,    0, 2202,    0, 1996,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 6387,  1997,     0,  1996,  1010, 10162,     0,  6016,     0,  2032,\n",
       "             0,  2136,     0,     0,     0,     0]),\n",
       " tensor([ 1010,  2331,     0,  2087,  3412, 15014,     0,  2041,     0,  2083,\n",
       "             0,  1998,     0,     0,     0,     0]),\n",
       " tensor([ 3375,  1012,     0,  3449, 14522,  2008,     0, 16690,     0,  2035,\n",
       "             0,  2007,     0,     0,     0,     0]),\n",
       " tensor([ 1998,  2130,     0,  2080,  1010, 14408,     0,  2015,     0,  1996,\n",
       "             0,  5587,     0,     0,     0,     0]),\n",
       " tensor([26502,  2065,     0, 15417,  1998, 21967,     0,  1997,     0, 22913,\n",
       "             0,  2100,     0,     0,     0,     0]),\n",
       " tensor([ 8078,  2027,     0,  1010,  2591,  2310,     0, 20752,     0,  1997,\n",
       "             0,  2841,     0,     0,     0,     0]),\n",
       " tensor([ 1010,  2106,     0, 23949, 20489, 12119,     0,  2008,     0,  5928,\n",
       "             0,  1012,     0,     0,     0,     0]),\n",
       " tensor([10424,  1010,     0,  5755,  1997,  1005,     0, 15176,     0,  2554,\n",
       "             0,  2059,     0,     0,     0,     0]),\n",
       " tensor([10448,  1037,     0,  1997, 15488,  1055,     0,  2673,     0,  1998,\n",
       "             0,  1037,     0,     0,     0,     0]),\n",
       " tensor([1997, 8009,    0, 3983, 3122, 9647,    0, 2013,    0, 2046,    0, 5920,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([1996, 1997,    0, 1011, 1999, 1012,    0, 2256,    0, 2070,    0, 7679,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([27127, 11721,     0,  2301,  7973,  1996,     0,  8072,     0,  4795,\n",
       "             0,  1037,     0,     0,     0,     0]),\n",
       " tensor([2003, 5243,    0, 2859, 1012, 4955,    0, 2000,    0, 1006,    0, 2610,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([1037, 1005,    0, 1010, 1000, 3640,    0, 2256,    0, 1998,    0, 4812,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([28190,  1055,     0,  1998,  2009, 16747,     0, 11311,     0, 25614,\n",
       "             0,  2006,     0,     0,     0,     0]),\n",
       " tensor([ 8297, 10473,     0,  2010,  1005, 20062,     0,  3001,     0,  1007,\n",
       "             0,  2873,     0,     0,     0,     0]),\n",
       " tensor([2000, 9219,    0, 3167, 1055, 2241,    0, 1010,    0, 8146,    0, 1998,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([9303, 4932,    0, 1010, 2590, 2006,    0, 2437,    0, 1012,    0, 2014,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([17471,  1996,     0,  2576,  2000,  3728,     0,  2204,     0, 17958,\n",
       "             0,  4686,     0,     0,     0,     0]),\n",
       " tensor([2078, 4303,    0, 1010, 2031, 3603,    0, 6550,    0, 2007,    0, 1012,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 1997,  2006,     0,  1998, 11240,  5491,     0,  2552,     0,  9078,\n",
       "             0,  2044,     0,     0,     0,     0]),\n",
       " tensor([ 1996,  1996,     0, 12465,  2121,  1010,     0,  2066,     0, 15185,\n",
       "             0,  1996,     0,     0,     0,     0]),\n",
       " tensor([ 2600, 16985,     0, 14163,  2290,  1998,     0, 17663,     0,  2594,\n",
       "             0,  2034,     0,     0,     0,     0]),\n",
       " tensor([ 1010,  7559,     0,  7741,  1005,  3397,     0,  2015,     0, 15966,\n",
       "             0,  4400,     0,     0,     0,     0]),\n",
       " tensor([2013, 2271,    0, 2015, 1055, 2434,    0, 1517,    0, 1998,    0, 1997,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 1996,  2217,     0, 12342, 12495,  6947,     0,  1998,     0,  8579,\n",
       "             0,  5213,     0,     0,     0,     0]),\n",
       " tensor([ 7587,  1012,     0,  1037, 18980,  1997,     0,  2919,     0,  2989,\n",
       "             0,  1998,     0,     0,     0,     0]),\n",
       " tensor([2190, 4698,    0, 2422, 6002, 2310,    0, 6550,    0, 7982,    0, 9940,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 1011, 20915,     0,  2006,  2800, 12119,     0,  2066,     0,  1010,\n",
       "             0,  1010,     0,     0,     0,     0]),\n",
       " tensor([4855, 2232,    0, 1037, 2005, 1005,    0, 9947,    0, 3243,    0, 5587,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([1998, 1998,    0, 2088, 2178, 1055,    0, 2015,    0, 9200,    0, 2100,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 4800, 11312,     0,  2008,  4245,  4216,     0,  1012,     0,  2028,\n",
       "             0,  5363,     0,     0,     0,     0]),\n",
       " tensor([1011, 2064,    0, 2200, 1997, 1998,    0, 2057,    0, 2851,    0, 2000,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 2400,  1005,     0,  2261,  8141,  7780,     0,  2064,     0,  2003,\n",
       "             0, 26944,     0,     0,     0,     0]),\n",
       " tensor([1011, 1056,    0, 2530, 1012, 1025,    0, 1523,    0, 1037,    0, 1996,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 3045,  3599,     0,  2545,  1000,  1996,     0,  4608,     0, 10433,\n",
       "             0,  3606,     0,     0,     0,     0]),\n",
       " tensor([3166, 4888,    0, 2031, 1011, 3964,    0, 1524,    0, 2135,    0, 2369,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 1997,  1037,     0,  2412,  1011, 17908,     0,  2060,     0, 14036,\n",
       "             0,  1996,     0,     0,     0,     0]),\n",
       " tensor([ 2559, 19124,     0,  5621,  5503,  2005,     0,  2111,     0,  1998,\n",
       "             0,  2331,     0,     0,     0,     0]),\n",
       " tensor([ 2005,  6101,     0,  5319, 10556,  1996,     0,  1521,     0, 20022,\n",
       "             0,  1517,     0,     0,     0,     0]),\n",
       " tensor([ 4862,  1012,     0,  1012, 12096,  2034,     0,  1055,     0, 18436,\n",
       "             0,  1998,     0,     0,     0,     0]),\n",
       " tensor([23544,  2750,     0,  4821, 18727,  2051,     0,  6699,     0, 10874,\n",
       "             0, 10229,     0,     0,     0,     0]),\n",
       " tensor([2072, 1996,    0, 1010, 1010, 1996,    0, 1996,    0, 1012,    0, 2008,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([1010, 6659,    0, 2083, 2137, 5606,    0, 2126,    0,  102,    0, 1996,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 7494, 10238,     0,  2010,  4467,  1997,     0,  2057,     0,     0,\n",
       "             0,  6192,     0,     0,     0,     0]),\n",
       " tensor([15409,  1010,     0, 16718,  2820,  2613,     0,  4608,     0,     0,\n",
       "             0,  2090,     0,     0,     0,     0]),\n",
       " tensor([ 1010, 14015,     0,  1998,   102,  1011,     0,  1037,     0,     0,\n",
       "             0,  9721,     0,     0,     0,     0]),\n",
       " tensor([2006, 1010,    0, 2010,    0, 2166,    0, 3147,    0,    0,    0, 1998,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([1996, 4698,    0, 3063,    0, 4481,    0, 1010,    0,    0,    0, 2293,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([15333, 20915,     0,  2854,     0,  6563,     0,  1998,     0,     0,\n",
       "             0,  2064,     0,     0,     0,     0]),\n",
       " tensor([6894, 2232,    0, 1010,    0, 2011,    0, 1996,    0,    0,    0, 2022,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([16288,  1010,     0, 17377,     0,  2310,     0,  8465,     0,     0,\n",
       "             0,  4795,     0,     0,     0,     0]),\n",
       " tensor([ 2346, 11312,     0, 12453,     0, 12119,     0,  1997,     0,     0,\n",
       "             0,  9291,     0,     0,     0,     0]),\n",
       " tensor([ 1998,  1010,     0,  2015,     0,  1012,     0, 12477,     0,     0,\n",
       "             0,  1012,     0,     0,     0,     0]),\n",
       " tensor([1996, 1998,    0, 1996,    0,  102,    0, 2030,    0,    0,    0, 1996,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([11939,  1996,     0,  2691,     0,     0,     0, 21660,     0,     0,\n",
       "             0,  6315,     0,     0,     0,     0]),\n",
       " tensor([ 1005,  2060,     0,  2598,     0,     0,     0,  2591,     0,     0,\n",
       "             0, 25289,     0,     0,     0,     0]),\n",
       " tensor([ 1055, 27668,     0,  7176,     0,     0,     0,  6911,     0,     0,\n",
       "             0,  1997,     0,     0,     0,     0]),\n",
       " tensor([2365, 3995,    0, 2149,    0,    0,    0, 2064,    0,    0,    0, 2611,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([1012, 5104,    0, 2035,    0,    0,    0, 2022,    0,    0,    0, 9021,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([ 102, 1997,    0, 1999,    0,    0,    0, 2166,    0,    0,    0, 2024,\n",
       "            0,    0,    0,    0]),\n",
       " tensor([  0, 102,   0, 102,   0,   0,   0, 102,   0,   0,   0, 102,   0,   0,\n",
       "           0,   0])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(ds['train']))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "\n",
    "# change later, if using virtual machine\n",
    "device ='cpu' #\"cuda\" if cuda.is_available() else \"cpu\"\n",
    "\n",
    "# seed the model before initializing weights so that your code is deterministic\n",
    "manual_seed(457)\n",
    "\n",
    "freeze_bert = False # change later\n",
    "batch_size = 1 # default\n",
    "epochs = 5 # default\n",
    "learning_rate = 1e-2 # default\n",
    "\n",
    "model = BERTGenreClassification(freeze_bert = freeze_bert).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 16])\n",
      "torch.Size([256, 16])\n",
      "torch.Size([303, 16])\n",
      "torch.Size([16, 303])\n",
      "torch.Size([16, 303])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 303]], which is output 0 of SigmoidBackward0, is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdev_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\OneDrive\\Documents\\Senior Year\\Natural Language Processing\\NLP-Project-\\BERTModel.py:81\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_dataloader, dev_dataloader, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     78\u001b[0m b_loss \u001b[38;5;241m=\u001b[39m loss_func(preds\u001b[38;5;241m.\u001b[39mfloat(), labels\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     79\u001b[0m e_loss \u001b[38;5;241m=\u001b[39m e_loss \u001b[38;5;241m+\u001b[39m b_loss\n\u001b[1;32m---> 81\u001b[0m \u001b[43mb_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     84\u001b[0m b_num \u001b[38;5;241m=\u001b[39m b_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [16, 303]], which is output 0 of SigmoidBackward0, is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "train_model(model= model, train_dataloader= ds['train'], \n",
    "            dev_dataloader= ds['validation'], epochs= epochs, learning_rate= learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m emb_length    \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      2\u001b[0m output_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLength of input embeddings: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memb_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "emb_length    = len(ds['train'][0]['input_ids'])\n",
    "output_length = len(ds['train'][0]['label'])\n",
    "\n",
    "print(f'Length of input embeddings: {emb_length}')\n",
    "print(f'Length of output layer: {output_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from util import BERT_preprocess\n",
    "bert_train = BERT_preprocess(bert_train, label_as_id)\n",
    "bert_val = BERT_preprocess(bert_val, label_as_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "bert_val.head()\n",
    "print(sum(bert_val.iloc[3, 6]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<lambda>() got an unexpected keyword argument 'batched'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# BERT data loaders and training\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#from util import BERT_preprocess\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#bert_train = BERT_preprocess(bert_train, label_as_id)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#bert_val = BERT_preprocess(bert_val, label_as_id)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# have to change these to implement from df, not from csv\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m val_dataloader \u001b[38;5;241m=\u001b[39m get_dataloader(bert_val, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# currently, embedding dimensions are wrong\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#train_model(model, train_dataloader, dev_dataloader, epochs, learning_rate)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\OneDrive\\Documents\\Senior Year\\Natural Language Processing\\NLP-Project-\\util.py:218\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(data, batch_size)\u001b[0m\n\u001b[0;32m    216\u001b[0m dataset \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    217\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 218\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;66;03m#dataset = data.map(lambda x: toke, batched = True)\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m#dataset = dataset.with_format(\"torch\")\u001b[39;00m\n\u001b[0;32m    223\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\frame.py:10120\u001b[0m, in \u001b[0;36mDataFrame.map\u001b[1;34m(self, func, na_action, **kwargs)\u001b[0m\n\u001b[0;32m  10117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[0;32m  10118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39m_map_values(func, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m> 10120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10033\u001b[0m )\n\u001b[1;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\apply.py:965\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 965\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\apply.py:981\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    980\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 981\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m    982\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    983\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    984\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    985\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\frame.py:10118\u001b[0m, in \u001b[0;36mDataFrame.map.<locals>.infer\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m  10117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfer\u001b[39m(x):\n\u001b[1;32m> 10118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\oscar\\anaconda3\\envs\\ml-0451\\lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: <lambda>() got an unexpected keyword argument 'batched'"
     ]
    }
   ],
   "source": [
    "# BERT data loaders and training\n",
    "#from util import BERT_preprocess\n",
    "#bert_train = BERT_preprocess(bert_train, label_as_id)\n",
    "#bert_val = BERT_preprocess(bert_val, label_as_id)\n",
    "\n",
    "# have to change these to implement from df, not from csv\n",
    "train_dataloader = get_dataloader(bert_train, batch_size=batch_size)\n",
    "val_dataloader = get_dataloader(bert_val, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# currently, embedding dimensions are wrong\n",
    "#train_model(model, train_dataloader, dev_dataloader, epochs, learning_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-0451",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
